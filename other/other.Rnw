% !Rnw root = ../mlcrash.Rnw

<<other, cache=FALSE, include=FALSE>>=
  opts_knit$set(self.contained=FALSE)
@

%%%%%%%%%%%%
\part{Other}
%%%%%%%%%%%%
\newthought{In this section I note some other techniques} one may come across and others that will provide additional insight into machine learning applications.

%%%
\section{Unsupervised Learning}
Unsupervised learning generally speaking involves techniques in which we are utilizing unlabeled data. In this case we have our typical set of features we are interested in, but no particular response to map them to.  In this situation we are more interested in the discovery of structure within the data.

\subsection{Clustering}
Many of the techniques used in unsupervised are commonly taught in various applied disciplines as various forms of "cluster" analysis.  The gist is we are seeking an unknown class structure rather than seeing how various inputs relate to a known class structure.  Common techniques include k-means, hierarchical clustering, and model based approaches (e.g. mixture models).

\subsection{Latent Variable Models}
\marginnote{\includegraphics[scale=.35]{images/lvmodel}}
Sometimes the desire is to reduce the dimensionality of the inputs to a more manageable set of information.  In this manner we are thinking that much of the data can be seen as having only a few sources of variability, often called latent variables or factors.  Again, this takes familiar forms such as principal components and ("exploratory") factor analysis, but would also include independence components analysis and partial least squares techniques.  Note also that these can be part of a supervised technique (e.g. principal components regression) or the main focus of analysis (as with latent variable models in structural equation modeling).

\subsection{Graphical Structure}
\marginnote {\begin{center}\includegraphics[scale=.35]{images/senategraph}\end{center} Example graph of the social network of senators based on data and filter at the following link. Node size is based on the \emph{betweeness} centrality measure, edge size the percent agreement (graph filtered to edges >= 65\%). Color is based on the clustering discovered within the graph. Zoom in as necessary, and note that you may need to turn off 'enhance thin lines' in your Adobe Page Display Preferences if using it as a viewer. \href{http://support.google.com/fusiontables/answer/2566732?hl=en&ref_topic=2572801}{link to data}.}
Other techniques are available to understand structure among observations or features.  Among the many approaches is the popular \emph{network analysis}, where we can obtain similarities among observations and examine visually the structure of those data points, where observations are placed closer together that are more similar in nature. In still other situations, we aren't so interested in the structure as we are in modeling the relationships and making predictions from the correlations of inputs.

\subsection{Imputation}
We can also use these techniques when we are missing data as a means to impute the missing values\sidenote{This and other techniques may fall under the broad heading of matrix completion.}.  While many are familiar with this problem and standard techniques for dealing with it, it may not be obvious that ML techniques may also be used.  For example, both k-nearest neighbors and random forest techniques have been applied to imputation. 

Beyond this we can infer values that are otherwise unavailable in a different sense.  Consider Netflix, Amazon and other sites that suggest various products based on what you already like or are interested in.  In this case the suggested products have missing values for the user which are imputed or inferred based on their available data and other consumers similar to them who have rated the product in question.  Such \emph{recommender systems} are widely used these days.

%%%
\section{Ensembles}
In many situations we can combine the information of multiple models to enhance prediction.  This can take place within a specific technique, e.g. random forests, or between models that utilize different techniques.  I will discuss some standard techniques, but there are a great variety of forms in which model combination might take place.

\subsection{Bagging}
\emph{Bagging}, or bootstrap aggregation, uses bootstrap sampling to create many data sets on which a procedure is then performed.  The final prediction is based on an average of all the predictions made for each observation.  In general, bagging helps reduce the variance while leaving bias unaffected. A conceptual outline of the procedure is provided.

\medskip
\noindent \emph{Model Generation} \\
For $B$ number of iterations:
\begin{enumerate}
  \item Sample $N$ observations with replacement $B$ times to create $B$ data sets of size $N$.
  \item Apply the learning technique to each of $B$ data sets to create $t$ models.
  \item Store the $t$ results.
\end{enumerate}
\emph{Classification} \\
For each of $t$ number of models:
\begin{enumerate}
  \item Predict the class of $N$ observations of the original data set.
  \item Return the class predicted most often.
\end{enumerate}

\subsection{Boosting}
With \emph{boosting} we take a different approach to refitting models.  Consider a classification task in which we start with a basic learner and apply it to the data of interest.  Next the learner is refit, but with more weight (importance) given to \emph{misclassified} observations.  This process is repeated until some stopping rule is reached.  An example of the AdaBoost algorithm is provided (in the following $\mathbb{I}$ is the indicator function).

\medskip
\small
\begin{itemize}
  \item Set initial weights $w_i$ to $1/N$.
  \item for $m=1:M$ \quad\{
    \item \qquad Fit a classifier $m$ with given weights to the data resulting in  \\ \qquad predictions $f^{(m)}_i$ that minimizes some loss function.
    \item \qquad Compute the error rate $err_m = \frac{\overset{N}{\underset{i=1}{\sum}} \mathbb{I}(y_i\ne f^{(m)}_i)}{\overset{N}{\underset{i=1}{\sum}}w^{(m)}_i}$
    \item \qquad Compute $\alpha_m = \log[(1-err_m)/err_m]$
    \item \qquad Set $w_i \leftarrow w_i\exp[\alpha_m \mathbb{I}(y_i\ne f^{(m)}_i)]$ \\
    \} \\
    \texttt{Return} $\sgn [\overset{M}{\underset{m=1}{\sum}}\alpha_m f^{(m)}]$
\end{itemize}
\normalsize

\medskip
Boosting can be applied to a variety of tasks and loss functions, and in general is highly resistant to overfitting.

\subsection{Stacking}
\emph{Stacking} is a method that can generalize beyond a single fitting technique, though it can be applied in a fashion similar to boosting for a single technique.  While the term can refer to a specific technique, here we will use it broadly to mean any method to combine models of different forms. Consider the four approaches we demonstrated earlier: k-nearest neighbors, neural net, random forest, and the support vector machine.  We saw that they do not have the same predictive accuracy, though they weren't bad in general. Perhaps by combining their respective efforts, we could get even better prediction than using any particular one. 

The issue then how we might combine them.  We really don't have to get too fancy with it, and can even use a simple voting scheme as in bagging. For each observation, note the predicted class on new data across models. The final prediction is the class that receives the most votes.  Another approach would be to use a weighted vote, where the votes are weighted by their respective accuracies.  

Another approach would use the predictions on the test set to create a data set of just the predicted probabilities from each learning scheme. We can then use this data to train a meta-learner using the test labels as the response.  With the final meta-learner chosen, we then retrain the original models on the entire data set (i.e. including the test data).  In this manner the initial models and the meta-learner are trained separately and you get to eventually use the entire data set to train the original models.  Now when new data becomes available, you feed them to the base level learners, get the predictions, and then feed the predictions to the meta-learner for the final prediction.

%%%
\section{Feature Selection \& Importance}
We hit on this topic some before, but much like there are a variety of ways to gauge performance, there are different approaches to select features and/or determine their importance. Invariably feature selection takes place from the outset when we choose what data to collect in the first place.  Hopefully guided by theory, in other cases it may be restricted by user input, privacy issues, time constraints and so forth.  But once we obtain the initial data set however, we may still want to trim the models under consideration.

In standard approaches we might have in the past used forward or other selection procedure, or perhaps some more explicit model comparison approach.  Concerning the content here, take for instance the lasso regularization procedure we spoke of earlier.  'Less important' variables may be shrunk entirely to zero, and thus feature selection is an inherent part of the process, and is useful in the face of many, many predictors, sometimes outnumbering our sample points. As another example, consider any particular approach where the importance metric might be something like the drop in accuracy when the variable is excluded.

Variable importance was given almost full weight in the discussion of typical applied research in the past, based on statistical significance results from a one-shot analysis, and virtually ignorant of prediction on new data.  We still have the ability to focus on feature performance with ML techniques, while shifting more of the focus toward prediction at the same time.  For the uninitiated, it might require new ways of thinking about how one measures importance though.

%%%
\section{Textual Analysis}
In some situations the data of interest is not in a typical matrix form but in the form of textual content, i.e. a corpus of documents (loosely defined).  In this case, much of the work (like in most analyses but perhaps even more so) will be in the data preparation, as text is rarely if ever in a ready-to-analyze state.  The eventual goals may include using the word usage in the prediction of an outcome, perhaps modeling the usage of select terms, or examining the structure of the term usage graphically as in a network model.  In addition, machine learning processes might be applied to sounds (acoustic data) to discern the speech characteristics and other information.

%%%
\section{Bayesian Approaches}
It should be noted that the approaches outlined in this document are couched in the frequentist tradition.  But one should be aware that many of the concepts and techniques would carry over into the Bayesian perspective, and even some machine learning techniques might only be feasible or make more sense within the Bayesian framework (e.g. online learning). 


%%%
\section{More Stuff}
Aside from what has already been noted, there still exists a great many applications for ML such as data set shift
\sidenote{Used when fundamental changes occur between the data a learner is trained on and the data coming in for further analysis.}, 
deep learning\sidenote{Learning at different levels of representation, e.g. from an image regarding a scene to the concepts used to describe it.}, 
semi-supervised learning\sidenote{Learning with both labeled and unlabeled data.}, 
online learning\sidenote{Learning from a continuous stream of data.}, and many more.  






