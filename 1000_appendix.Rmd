# Appendix
 
 
## Bias Variance Demo
 
```{r biasvar_demo, eval=FALSE}
set.seed(123)
x = runif(1000)
ytrue = sin(3*pi*x)
basedat = cbind(x,ytrue)[order(x),]
 
gendatfunc = function(noise=.5, n=1000){
  x = runif(n)
  y = sin(3*pi*x) + rnorm(n, sd=noise) # truth
  d = cbind(x, y)
  d
}
 
gendat = replicate(100, gendatfunc(n=100))
str(gendat)
 
library(kernlab)
 
rbf1 = apply(gendat, 3, 
             function(d) predict(gausspr(y~x, data=data.frame(d), kpar=list(sigma=.5)), 
                                 newdata = data.frame(x), type='response'))
rbf2 = apply(gendat, 3, 
             function(d) predict(gausspr(y~x, data=data.frame(d)), 
                                 newdata = data.frame(x), type='response') )
 
library(ggplot2); library(tidyverse); library(gridExtra)
 
rbf1_samp = rbf1 %>% 
  data.frame %>% 
  cbind(x, .) %>%
  slice(sample(1:100, 25)) %>% 
  gather(key=sample, value=yhat, -x)
 
rbf2_samp = rbf2 %>% 
  data.frame %>% 
  cbind(x, .) %>%
  slice(sample(1:100, 25)) %>% 
  gather(key=sample, value=yhat, -x)
 
g1 = ggplot(data=data.frame(basedat)) +
  geom_blank() +
  geom_line(aes(x=x, y=yhat, group=sample), color='#ff5500', alpha=.25, data=rbf1_samp) +
  ylim(c(-1.5, 1.5)) +
  labs(y='', title='Low Variance') + 
  lazerhawk::theme_trueMinimal() +
    theme(
    legend.key = ggplot2::element_rect(fill='#fffff8', colour = NA),
    legend.background = ggplot2::element_rect(fill='#fffff8', colour = NA),
    panel.background = ggplot2::element_blank(),
    panel.grid = ggplot2::element_blank(),
    strip.background = ggplot2::element_blank(),
    plot.background = ggplot2::element_rect(fill = "#fffff8", colour = NA)
  )
 
g2 = ggplot(data=data.frame(basedat)) +
  geom_line(aes(x=x, y=ytrue), color='#00aaff') +
  geom_line(aes(x=x, y=yhat), color='#ff5500', data.frame(yhat=rowMeans(rbf1))) +
  ylim(c(-1.5, 1.5)) +
  labs(y='', title='High Bias') + 
  lazerhawk::theme_trueMinimal() +
    theme(
    legend.key = ggplot2::element_rect(fill='#fffff8', colour = NA),
    legend.background = ggplot2::element_rect(fill='#fffff8', colour = NA),
    panel.background = ggplot2::element_blank(),
    panel.grid = ggplot2::element_blank(),
    strip.background = ggplot2::element_blank(),
    plot.background = ggplot2::element_rect(fill = "#fffff8", colour = NA)
  )
 
g3 = ggplot(data=data.frame(basedat)) +
  geom_blank() +
  geom_line(aes(x=x, y=yhat, group=sample), color='#ff5500', alpha=.25, data=rbf2_samp) +
    ylim(c(-1.5, 1.5)) +
  labs(y='', title='High Variance') + 
  lazerhawk::theme_trueMinimal() +
    theme(
    legend.key = ggplot2::element_rect(fill='#fffff8', colour = NA),
    legend.background = ggplot2::element_rect(fill='#fffff8', colour = NA),
    panel.background = ggplot2::element_blank(),
    panel.grid = ggplot2::element_blank(),
    strip.background = ggplot2::element_blank(),
    plot.background = ggplot2::element_rect(fill = "#fffff8", colour = NA)
  )
 
g4 = ggplot(data=data.frame(basedat)) +
  geom_line(aes(x=x, y=ytrue), color='#00aaff') +
  geom_line(aes(x=x, y=yhat), color='#ff5500', data.frame(yhat=rowMeans(rbf2))) +
  ylim(c(-1.5, 1.5)) +
  labs(y='', title='Low Bias') + 
  lazerhawk::theme_trueMinimal() +
    theme(
    legend.key = ggplot2::element_rect(fill='#fffff8', colour = NA),
    legend.background = ggplot2::element_rect(fill='#fffff8', colour = NA),
    panel.background = ggplot2::element_blank(),
    panel.grid = ggplot2::element_blank(),
    strip.background = ggplot2::element_blank(),
    plot.background = ggplot2::element_rect(fill = "#fffff8", colour = NA)
  )
 
grid.arrange(g1, g2, g3, g4, ncol=2)
```
 
 
 
## Programming Languages
 
### R

Demonstrations for this document were done with R, and specifically the <span class="pack">caret</span> package. I would highly recommend using it for your own needs, as it makes a lot the ML process simpler, while providing access to whatever technique you want to use, even while it comes with the ability to use hundreds of approaches out of the box.  

#### Deep learning example

The following is a deep learning example using the same data from the previous examples, using <span class="pack">keras</span>.  The package is an R wrapper for the <span class="pack">keras</span> module in Python, which itself is a wrapper for <span class="pack">tensorflow</span>.  For more on using TensorFlow with R, check out [RStudio's documentation](https://tensorflow.rstudio.com/).

This is a <span class="emph">sequential neural net</span> with three layers.  I also add some <span class="emph">dropout</span> at each layer, which, given the material presented, you can think of as a means of regularization to avoid overfitting[^onepointo].  Note that keras works a bit differently, in that its objects are mutable, hence the there is no reassignment of the <span class="objclass">model</span>.  This R implementation also provides some nice visualizations to keep your interest while it runs. 

This is just a starting point, so don't expect any better performance than those shown previously, but it'll give you something to play with. Note that there is plenty to learn, and such models require copious amounts of tuning to work well.

```{r rdl, eval=FALSE}
library(keras)

X_train = model.matrix(lm(good ~ . -1, data=wine_train)) %>% scale()
y_train = as.numeric(wine_train$good) - 1
X_test = model.matrix(lm(good ~ . -1, data=wine_test)) %>% scale()
y_test = as.numeric(wine_test$good) - 1

model = keras_model_sequential() 

model %>%
  layer_dense(units = 256, activation = 'relu', input_shape = c(9)) %>%
  layer_dropout(rate = 0.4) %>% 
  layer_dense(units = 128, activation = 'relu') %>%
  layer_dropout(rate = 0.3) %>%
  layer_dense(units = 64, activation = 'relu') %>%
  layer_dropout(rate = 0.2) %>%
  layer_dense(units = 1, activation = 'sigmoid')

summary(model)

model %>% compile(
  optimizer = 'nadam',
  loss = "binary_crossentropy",
  metrics = "accuracy"
)

history = model %>% 
  fit(X_train, 
      y_train, 
      epochs = 10000, 
      validation_split = 0.1,
      verbose = 1,
      view_metrics = 1)

plot(history)

# model %>% evaluate(X_test, y_test)

ypred = model %>% predict_classes(X_test)
caret::confusionMatrix(ypred, y_test, positive='1')
```




### Python

If your data fits on your machine and/or your analysis time is less than a couple hours, R is hands down the easiest to use to go from data to document, including if that document is an interactive website.  That said, R probably isn't even the most popular ML tool, because in many situations we have a lot more data, or simply need the predictions without frills and as fast as possible[^rfaster].  As such Python is the de facto standard in such situations, and probably the most popular development environment for machine learning.

One can start with the <span class="pack">scikit-learn</span> module, using it much in the same way as was demonstrated with <span class="pack">caret</span>.  It will get you very far, but for some situations, you may need more heavy-duty options like <span class="pack">tensorflow</span>, <span class="pack">pytorch</span>, etc.

```{python pyrf, eval=T, cache=TRUE}
import sklearn as sk
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.ensemble import RandomForestClassifier
# from os import chdir            # if desired
# chdir('your_directory')

# import data
wine = pd.read_csv('data/wine.csv')

# data preprocessing
np.random.seed(1234)
X = wine.drop(['free.sulfur.dioxide', 'density', 'quality', 'color', 'white','good'], axis=1)
X = MinMaxScaler().fit_transform(X)  # by default on 0, 1 scale
y = wine['good']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# train model
rf = RandomForestClassifier(n_estimators=1000)
rf_train = rf.fit(X_train, y_train)

# get test predictions
rf_predict = rf_train.predict(X_test)

# create confusion matrix, and accuracy
cm = sk.metrics.confusion_matrix(y_test,rf_predict)
cm_prob = cm / np.sum(cm)  # as probs
print(cm_prob)

acc = sk.metrics.accuracy_score(y_test, rf_predict)
acc = pd.DataFrame(np.array([acc]), columns=['Accuracy'])
print(acc)
```

#### Deep learning example

Here is a bit of code to get you started with a scikit approach to deep learning. This is a 'deep neural net' with seven layers.  I also add some 'dropout'.  This isn't too viable a model as far as settings go, and won't do any better than those shown previously[^dnn], but it'll run fine on your machine so you can at least play with it.

```{python dl, eval=F}
import tensorflow.contrib.learn as skflow
from sklearn import metrics

feats = skflow.infer_real_valued_columns_from_input(X_train)

classifier_tf = skflow.DNNClassifier(feature_columns=feats, 
                                     hidden_units=[50, 50, 50, 40, 30, 20, 10], 
                                     dropout=.2,
                                     n_classes=2)
classifier_tf.fit(X_train, y_train, steps=10000)
predictions = list(classifier_tf.predict(X_test, as_iterable=True))
score = metrics.accuracy_score(y_test, predictions)
print("Accuracy: %f" % score)
```


### Other

I wouldn't recommend a proprietary tool when better open source tools are available, but I will say **Matlab** is also very popular in machine learning, and specific areas like image processing. **Julia** has been coming along as well.  Of course, for maximum speed people still prefer lower level languages like **C++**, **Java**, and **Go**, and many of the Python modules are interfaces to such lower-level libraries. And for whatever reason, people are reinventing ML wheels in languages like **Javascript** and others.  See the [awesome list](https://github.com/josephmisiti/awesome-machine-learning) for more.

What I can't recommend is a traditional statistics package like SPSS, SAS, or Stata. Not only did they miss this boat by well over a decade, their offerings are slim and less capable.  It seems SAS is probably the only one that's made serious effort here, and it has some audience in the business world due to its long entrenchment there.  And you don't have to take my word for it- here's a comparison of trends at [indeed.com](https://www.indeed.com/jobtrends/q-R-and-%28%22machine-learning%22-or-%22data-science%22%29-q-python-and-%28%22machine-learning%22-or-%22data-science%22%29-q-SAS-and-%28%22machine-learning%22-or-%22data-science%22%29-q-SPSS-and-%28%22machine-learning%22-or-%22data-science%22%29-q-Stata-and-%28%22machine-learning%22-or-%22data-science%22%29-q-Matlab-and-%28%22machine-learning%22-or-%22data-science%22%29.html).


## Brief Glossary of Common Terms


<span class="emph">bias</span>: could mean the intercept (e.g. in neural nets), typically refers to the bias in bias-variance decomposition

<span class="emph">classifier</span>: specific model or technique (i.e. function) that maps observations to classes

<span class="emph">confusion matrix</span>: a table of predicted class membership vs. true class membership

<span class="emph">hypothesis</span>: a specific model $h(x)$ of all possible in the hypothesis space $\mathcal{H}$

<span class="emph">input, feature, attribute</span>: independent variable, explanatory variable, covariate, predictor variable, column

<span class="emph">instance, example</span>: observation, row

<span class="emph">learning</span>: model fitting

<span class="emph">machine learning</span>: a form of statistics utilizing various algorithms with a goal to generalize to new data situations

<span class="emph">regularization, penalization, shrinkage</span>: The process of adding a penalty to the size of coefficients, thus shrinking them towards zero but resulting in less overfitting (at an increase to bias)

<span class="emph">supervised</span>: has a dependent variable

<span class="emph">target, label</span>: dependent variable, response, the outcome of interest

<span class="emph">unsupervised</span>: no dependent variable (or rather, only dependent variables); think clustering, PCA etc.

<span class="emph">weights</span>:  coefficients, parameters


# References


[^rfaster]: Actually, for this <span class="func">rf.fit</span> was slower than the default <span class="func">randomForest</span> function in R by about a second under similar settings.

[^dnn]: Just for giggles I let that particular one go for 100000 steps and it finally started to get into > 81% on the test set. 

[^onepointo]: You can obtain near perfect accuracy on the training set without dropout, but will do no better at test.