# Appendix


## Bias Variance Demo

```{r biasvar_demo, eval=FALSE}
set.seed(123)
x = runif(1000)
ytrue = sin(3*pi*x)
basedat = cbind(x,ytrue)[order(x),]

gendatfunc = function(noise=.5, n=1000){
  x = runif(n)
  y = sin(3*pi*x) + rnorm(n, sd=noise) # truth
  d = cbind(x, y)
  d
}

gendat = replicate(100, gendatfunc(n=100))
str(gendat)

library(kernlab)

rbf1 = apply(gendat, 3, 
             function(d) predict(gausspr(y~x, data=data.frame(d), kpar=list(sigma=.5)), 
                                 newdata = data.frame(x), type='response'))
rbf2 = apply(gendat, 3, 
             function(d) predict(gausspr(y~x, data=data.frame(d)), 
                                 newdata = data.frame(x), type='response') )

library(ggplot2); library(tidyverse); library(gridExtra)

rbf1_samp = rbf1 %>% 
  data.frame %>% 
  cbind(x, .) %>%
  slice(sample(1:100, 25)) %>% 
  gather(key=sample, value=yhat, -x)

rbf2_samp = rbf2 %>% 
  data.frame %>% 
  cbind(x, .) %>%
  slice(sample(1:100, 25)) %>% 
  gather(key=sample, value=yhat, -x)

g1 = ggplot(data=data.frame(basedat)) +
  geom_blank() +
  geom_line(aes(x=x, y=yhat, group=sample), color='#ff5503', alpha=.25, data=rbf1_samp) +
  ylim(c(-1.5, 1.5)) +
  labs(y='', title='Low Variance') + 
  lazerhawk::theme_trueMinimal()

g2 = ggplot(data=data.frame(basedat)) +
  geom_line(aes(x=x, y=ytrue), color='#03b3ff') +
  geom_line(aes(x=x, y=yhat), color='#ff5503', data.frame(yhat=rowMeans(rbf1))) +
  ylim(c(-1.5, 1.5)) +
  labs(y='', title='High Bias') + 
  lazerhawk::theme_trueMinimal()

g3 = ggplot(data=data.frame(basedat)) +
  geom_blank() +
  geom_line(aes(x=x, y=yhat, group=sample), color='#ff5503', alpha=.25, data=rbf2_samp) +
  ylim(c(-1.5, 1.5)) +
  labs(y='', title='High Variance') + 
  lazerhawk::theme_trueMinimal()

g4 = ggplot(data=data.frame(basedat)) +
  geom_line(aes(x=x, y=ytrue), color='#03b3ff') +
  geom_line(aes(x=x, y=yhat), color='#ff5503', data.frame(yhat=rowMeans(rbf2))) +
  ylim(c(-1.5, 1.5)) +
  labs(y='', title='Low Bias') + 
  lazerhawk::theme_trueMinimal()

grid.arrange(g1, g2, g3, g4, ncol=2)
```



## Programming Languages

### R

Demonstrations for this document were done with R, and specifically the <span class="pack">caret</span> package. I would highly recommend using it for your own needs, as it makes a lot the ML process simpler, while providing access to whatever technique you want to use, even while it comes with the ability to use hundreds of approaches out of the box.  

### Python

If your data fits on your machine and/or your analysis time is less than a couple hours, R is hands down the easiest to use to go from data to document, including if that document is an interactive website.  That said, R probably isn't even the most popular ML tool, because in many situations we have a lot more data, or simply need the predictions without frills and as fast as possible[^rfaster].  As such Python is the de facto standard in such situations, and probably the most popular development environment for machine learning.

One can start with the <span class="pack">scikit-learn</span> module, using it much in the same way as was demonstrated with caret.  It will get you very far, but for some situations, you may need more heavy duty options like <span class="pack">tensorflow</span>, <span class="pack">Theano</span>, etc.

```{python pyrf, eval=T, echo=-c(7,10)}
import sklearn as sk
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.ensemble import RandomForestClassifier
# from os import chdir

# import data
# chdir('D:\Documents\Stats\Repositories\Docs\introduction-to-machine-learning')
wine = pd.read_csv('data/wine.csv')

# data preprocessing
np.random.seed(1234)
X = wine.drop(['free.sulfur.dioxide', 'density', 'quality', 'color', 'white','good'], axis=1)
X = MinMaxScaler().fit_transform(X)  # by default on 0, 1 scale
y = wine['good']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# train model
rf = RandomForestClassifier(n_estimators=1000)
rf_train = rf.fit(X_train, y_train)

# get test predictions
rf_predict = rf_train.predict(X_test)

# create confusion matrix, and accuracy
cm = sk.metrics.confusion_matrix(y_test,rf_predict)
cm_prob = cm / np.sum(cm)  # as probs
print(cm_prob)

acc = sk.metrics.accuracy_score(y_test, rf_predict)
acc = pd.DataFrame(np.array([acc]), columns=['Accuracy'])
print(acc)
```


### Other

I wouldn't recommend a proprietary tool when better open source tools are available, but I will say **Matlab** is also very popular in machine learning, and specific areas like image processing. **Julia** has been coming along as well.  Of course for maximum speed people still prefer lower level languages like **C++**, **Java**, and **Fortran**. And for whatever reason, people are reinventing ML wheels in languages like **Javascript** and others.  See the [awesome list](https://github.com/josephmisiti/awesome-machine-learning) for more.

What I can't recommend is a traditional statistics package like SPSS, SAS, or Stata. Not only did they miss this boat by over a decade, their offerings are slim and less capable.  It seems SAS is probably the only one that's made serious effort here, and it has some audience in the business world due to its long entrenchment there.  And you don't have to take my word for it- here's a comparison of trends at [indeed.com](https://www.indeed.com/jobtrends/q-R-and-%28%22machine-learning%22-or-%22data-science%22%29-q-python-and-%28%22machine-learning%22-or-%22data-science%22%29-q-SAS-and-%28%22machine-learning%22-or-%22data-science%22%29-q-SPSS-and-%28%22machine-learning%22-or-%22data-science%22%29-q-Stata-and-%28%22machine-learning%22-or-%22data-science%22%29-q-Matlab-and-%28%22machine-learning%22-or-%22data-science%22%29.html).


## Brief Glossary of Common Terms

The following lists some common ML terms, and their more traditional statistical counterpart.

<span class="emph">bias</span>: could mean the intercept (e.g. in neural nets), typically refers to the bias in bias-variance decomposition

<span class="emph">regularization, penalization, shrinkage</span>: The process of adding a penalty to the size of coefficients, thus shrinking them towards zero but resulting in less overfitting (at an increase to bias)

<span class="emph">classifier</span>: specific model or technique (i.e. function) that maps observations to classes

<span class="emph">confusion matrix</span>: a table of predicted class membership vs. true class membership

<span class="emph">hypothesis</span>: a specific model $h(x)$ of all possible in the hypothesis space $\mathcal{H}$

<span class="emph">input, feature, attribute</span>: independent variable, explanatory variable, covariate, predictor variable, column

<span class="emph">instance, example</span>: observation, row

<span class="emph">learning</span>: model fitting

<span class="emph">machine learning</span>: a form of statistics utilizing various algorithms with a goal to generalize to new data situations

<span class="emph">supervised</span>: has a target variable

<span class="emph">target, label</span>: dependent variable, response, the outcome of interest

<span class="emph">unsupervised</span>: no target variable; think clustering, PCA etc.

<span class="emph">weights</span>: coefficients, parameters

<span class="emph">`NULL`</span>: statistical significance (you will not be concerning yourself with p-values in ML)



[^rfaster]: Actually for this <span class="func">rf.fit</span> was slower than the default <span class="func">randomForest</span> function in R by about a second under similar settings.