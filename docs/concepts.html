<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Machine Learning</title>
  <meta name="description" content="This document provides an introduction to machine learning for applied researchers. While conceptual in nature, demonstrations are provided for several common machine learning approaches of a supervised nature. In addition, all the R examples, which utilize the caret package, are also provided in Python via scikit-learn.">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="Machine Learning" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://m-clark.github.io/introduction-to-machine-learning/" />
  <meta property="og:image" content="https://m-clark.github.io/introduction-to-machine-learning/img/nineteeneightyR.png" />
  <meta property="og:description" content="This document provides an introduction to machine learning for applied researchers. While conceptual in nature, demonstrations are provided for several common machine learning approaches of a supervised nature. In addition, all the R examples, which utilize the caret package, are also provided in Python via scikit-learn." />
  <meta name="github-repo" content="m-clark/introduction-to-machine-learning/" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Machine Learning" />
  
  <meta name="twitter:description" content="This document provides an introduction to machine learning for applied researchers. While conceptual in nature, demonstrations are provided for several common machine learning approaches of a supervised nature. In addition, all the R examples, which utilize the caret package, are also provided in Python via scikit-learn." />
  <meta name="twitter:image" content="https://m-clark.github.io/introduction-to-machine-learning/img/nineteeneightyR.png" />



<meta name="date" content="2018-03-23">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="introduction.html">
<link rel="next" href="opening-the-black-box.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="libs/htmlwidgets-1.0/htmlwidgets.js"></script>
<script src="libs/datatables-binding-0.2/datatables.js"></script>
<link href="libs/dt-core-1.10.12/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="libs/dt-core-1.10.12/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="libs/dt-core-1.10.12/js/jquery.dataTables.min.js"></script>
<script src="libs/d3-3.5.3/./d3.min.js"></script>
<link href="libs/d3heatmapcore-0.0.0/heatmapcore.css" rel="stylesheet" />
<script src="libs/d3heatmapcore-0.0.0/heatmapcore.js"></script>
<script src="libs/d3-tip-0.6.6/index.js"></script>
<script src="libs/d3heatmap-binding-0.6.1.1/d3heatmap.js"></script>
<script src="libs/viz-0.3/viz.js"></script>
<link href="libs/DiagrammeR-styles-0.2/styles.css" rel="stylesheet" />
<script src="libs/grViz-binding-0.9.2/grViz.js"></script>


<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="css/book.css" type="text/css" />
<link rel="stylesheet" href="css/standard_html.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class='before'><a href="https://m-clark.github.io/">Machine Learning</span></a></li>

<li class="divider"></li>
<li><a href="index.html#section"></a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i>Introduction</a><ul>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#explanation-prediction"><i class="fa fa-check"></i>Explanation &amp; Prediction</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#terminology"><i class="fa fa-check"></i>Terminology</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#supervised-vs.unsupervised"><i class="fa fa-check"></i>Supervised vs. Unsupervised</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#tools-you-already-have"><i class="fa fa-check"></i>Tools you already have</a><ul>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#the-standard-linear-model"><i class="fa fa-check"></i>The Standard Linear Model</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#logistic-regression"><i class="fa fa-check"></i>Logistic Regression</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#expansions-of-those-tools"><i class="fa fa-check"></i>Expansions of Those Tools</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="concepts.html"><a href="concepts.html"><i class="fa fa-check"></i>Concepts</a><ul>
<li class="chapter" data-level="" data-path="concepts.html"><a href="concepts.html#loss-functions"><i class="fa fa-check"></i>Loss Functions</a><ul>
<li class="chapter" data-level="" data-path="concepts.html"><a href="concepts.html#continuous-outcomes"><i class="fa fa-check"></i>Continuous Outcomes</a></li>
<li class="chapter" data-level="" data-path="concepts.html"><a href="concepts.html#categorical-outcomes"><i class="fa fa-check"></i>Categorical Outcomes</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="concepts.html"><a href="concepts.html#regularization"><i class="fa fa-check"></i>Regularization</a><ul>
<li class="chapter" data-level="" data-path="concepts.html"><a href="concepts.html#r-example-1"><i class="fa fa-check"></i>R Example</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="concepts.html"><a href="concepts.html#bias-variance-tradeoff"><i class="fa fa-check"></i>Bias-Variance Tradeoff</a><ul>
<li class="chapter" data-level="" data-path="concepts.html"><a href="concepts.html#bias-variance"><i class="fa fa-check"></i>Bias &amp; Variance</a></li>
<li class="chapter" data-level="" data-path="concepts.html"><a href="concepts.html#the-tradeoff"><i class="fa fa-check"></i>The Tradeoff</a></li>
<li><a href="concepts.html#diagnosing-bias-variance-issues-possible-solutions">Diagnosing Bias-Variance Issues <em>&amp;</em> Possible Solutions</a></li>
<li class="chapter" data-level="" data-path="concepts.html"><a href="concepts.html#bias-variance-summary"><i class="fa fa-check"></i>Bias-Variance Summary</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="concepts.html"><a href="concepts.html#cross-validation"><i class="fa fa-check"></i>Cross-Validation</a><ul>
<li class="chapter" data-level="" data-path="concepts.html"><a href="concepts.html#adding-another-validation-set"><i class="fa fa-check"></i>Adding Another Validation Set</a></li>
<li class="chapter" data-level="" data-path="concepts.html"><a href="concepts.html#k-fold-cross-validation"><i class="fa fa-check"></i>K-fold Cross-Validation</a></li>
<li class="chapter" data-level="" data-path="concepts.html"><a href="concepts.html#bootstrap"><i class="fa fa-check"></i>Bootstrap</a></li>
<li class="chapter" data-level="" data-path="concepts.html"><a href="concepts.html#other-stuff"><i class="fa fa-check"></i>Other Stuff</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="opening-the-black-box.html"><a href="opening-the-black-box.html"><i class="fa fa-check"></i>Opening the Black Box</a><ul>
<li class="chapter" data-level="" data-path="opening-the-black-box.html"><a href="opening-the-black-box.html#process-overview"><i class="fa fa-check"></i>Process Overview</a><ul>
<li class="chapter" data-level="" data-path="opening-the-black-box.html"><a href="opening-the-black-box.html#data-preparation"><i class="fa fa-check"></i>Data Preparation</a></li>
<li class="chapter" data-level="" data-path="opening-the-black-box.html"><a href="opening-the-black-box.html#model-selection"><i class="fa fa-check"></i>Model Selection</a></li>
<li class="chapter" data-level="" data-path="opening-the-black-box.html"><a href="opening-the-black-box.html#model-assessment"><i class="fa fa-check"></i>Model Assessment</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="opening-the-black-box.html"><a href="opening-the-black-box.html#the-dataset"><i class="fa fa-check"></i>The Dataset</a></li>
<li class="chapter" data-level="" data-path="opening-the-black-box.html"><a href="opening-the-black-box.html#r-implementation"><i class="fa fa-check"></i>R Implementation</a></li>
<li class="chapter" data-level="" data-path="opening-the-black-box.html"><a href="opening-the-black-box.html#feature-selection-the-data-partition"><i class="fa fa-check"></i>Feature Selection &amp; The Data Partition</a></li>
<li class="chapter" data-level="" data-path="opening-the-black-box.html"><a href="opening-the-black-box.html#regularized-regression"><i class="fa fa-check"></i>Regularized regression</a><ul>
<li class="chapter" data-level="" data-path="opening-the-black-box.html"><a href="opening-the-black-box.html#strengths-weaknesses"><i class="fa fa-check"></i>Strengths &amp; Weaknesses</a></li>
<li class="chapter" data-level="" data-path="opening-the-black-box.html"><a href="opening-the-black-box.html#final-thoughts"><i class="fa fa-check"></i>Final Thoughts</a></li>
</ul></li>
<li><a href="opening-the-black-box.html#k-nearest-neighbors"><span class="math inline">\(k\)</span>-nearest Neighbors</a><ul>
<li class="chapter" data-level="" data-path="opening-the-black-box.html"><a href="opening-the-black-box.html#strengths-weaknesses-1"><i class="fa fa-check"></i>Strengths &amp; Weaknesses</a></li>
<li class="chapter" data-level="" data-path="opening-the-black-box.html"><a href="opening-the-black-box.html#final-thoughts-1"><i class="fa fa-check"></i>Final Thoughts</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="opening-the-black-box.html"><a href="opening-the-black-box.html#neural-networks"><i class="fa fa-check"></i>Neural Networks</a><ul>
<li class="chapter" data-level="" data-path="opening-the-black-box.html"><a href="opening-the-black-box.html#strengths-weaknesses-2"><i class="fa fa-check"></i>Strengths &amp; Weaknesses</a></li>
<li class="chapter" data-level="" data-path="opening-the-black-box.html"><a href="opening-the-black-box.html#final-thoughts-2"><i class="fa fa-check"></i>Final Thoughts</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="opening-the-black-box.html"><a href="opening-the-black-box.html#trees-forests"><i class="fa fa-check"></i>Trees &amp; Forests</a><ul>
<li class="chapter" data-level="" data-path="opening-the-black-box.html"><a href="opening-the-black-box.html#strengths-weaknesses-3"><i class="fa fa-check"></i>Strengths &amp; Weaknesses</a></li>
<li class="chapter" data-level="" data-path="opening-the-black-box.html"><a href="opening-the-black-box.html#final-thoughts-3"><i class="fa fa-check"></i>Final Thoughts</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="opening-the-black-box.html"><a href="opening-the-black-box.html#support-vector-machines"><i class="fa fa-check"></i>Support Vector Machines</a><ul>
<li class="chapter" data-level="" data-path="opening-the-black-box.html"><a href="opening-the-black-box.html#strengths-weaknesses-4"><i class="fa fa-check"></i>Strengths &amp; Weaknesses</a></li>
<li class="chapter" data-level="" data-path="opening-the-black-box.html"><a href="opening-the-black-box.html#final-thoughts-4"><i class="fa fa-check"></i>Final Thoughts</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="wrap-up.html"><a href="wrap-up.html"><i class="fa fa-check"></i>Wrap-up</a><ul>
<li class="chapter" data-level="" data-path="wrap-up.html"><a href="wrap-up.html#unsupervised-learning"><i class="fa fa-check"></i>Unsupervised Learning</a><ul>
<li class="chapter" data-level="" data-path="wrap-up.html"><a href="wrap-up.html#clustering"><i class="fa fa-check"></i>Clustering</a></li>
<li class="chapter" data-level="" data-path="wrap-up.html"><a href="wrap-up.html#latent-variable-models"><i class="fa fa-check"></i>Latent Variable Models</a></li>
<li class="chapter" data-level="" data-path="wrap-up.html"><a href="wrap-up.html#graphical-structure"><i class="fa fa-check"></i>Graphical Structure</a></li>
<li class="chapter" data-level="" data-path="wrap-up.html"><a href="wrap-up.html#imputation"><i class="fa fa-check"></i>Imputation</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="wrap-up.html"><a href="wrap-up.html#ensembles"><i class="fa fa-check"></i>Ensembles</a><ul>
<li class="chapter" data-level="" data-path="wrap-up.html"><a href="wrap-up.html#bagging"><i class="fa fa-check"></i>Bagging</a></li>
<li class="chapter" data-level="" data-path="wrap-up.html"><a href="wrap-up.html#boosting"><i class="fa fa-check"></i>Boosting</a></li>
<li class="chapter" data-level="" data-path="wrap-up.html"><a href="wrap-up.html#stacking"><i class="fa fa-check"></i>Stacking</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="wrap-up.html"><a href="wrap-up.html#deep-learning"><i class="fa fa-check"></i>Deep Learning</a></li>
<li class="chapter" data-level="" data-path="wrap-up.html"><a href="wrap-up.html#feature-selection-importance"><i class="fa fa-check"></i>Feature Selection &amp; Importance</a></li>
<li class="chapter" data-level="" data-path="wrap-up.html"><a href="wrap-up.html#natural-language-processingtext-analysis"><i class="fa fa-check"></i>Natural language processing/Text Analysis</a></li>
<li class="chapter" data-level="" data-path="wrap-up.html"><a href="wrap-up.html#bayesian-approaches"><i class="fa fa-check"></i>Bayesian Approaches</a></li>
<li class="chapter" data-level="" data-path="wrap-up.html"><a href="wrap-up.html#more-stuff"><i class="fa fa-check"></i>More Stuff</a></li>
<li class="chapter" data-level="" data-path="wrap-up.html"><a href="wrap-up.html#summary"><i class="fa fa-check"></i>Summary</a><ul>
<li class="chapter" data-level="" data-path="wrap-up.html"><a href="wrap-up.html#cautionary-notes"><i class="fa fa-check"></i>Cautionary Notes</a></li>
<li class="chapter" data-level="" data-path="wrap-up.html"><a href="wrap-up.html#some-guidelines"><i class="fa fa-check"></i>Some Guidelines</a></li>
<li class="chapter" data-level="" data-path="wrap-up.html"><a href="wrap-up.html#conclusion"><i class="fa fa-check"></i>Conclusion</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i>Appendix</a><ul>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html#bias-variance-demo"><i class="fa fa-check"></i>Bias Variance Demo</a></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html#programming-languages"><i class="fa fa-check"></i>Programming Languages</a><ul>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html#r"><i class="fa fa-check"></i>R</a></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html#python"><i class="fa fa-check"></i>Python</a></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html#other"><i class="fa fa-check"></i>Other</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html#brief-glossary-of-common-terms"><i class="fa fa-check"></i>Brief Glossary of Common Terms</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li class='after'><a href="https://m-clark.github.io/"><img src="img/mc.png" style="width:50%; padding:0px 0; display:block; margin: 0 auto;" alt="MC logo"></a></li>
<li class='after'><a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/"><img alt="Creative Commons License" style="width:50%; border-width:0; display:block; margin: 0 auto;" src="img/ccbysa.png" /></a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./"><span style="font-size:125%; font-family:Roboto; font-style:normal">Machine Learning</span></a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="concepts" class="section level1">
<h1>Concepts</h1>
<p><span class="newthought">Given a set of predictor variables</span> <span class="math inline">\(X\)</span> and some target <span class="math inline">\(y\)</span>, we look for some function, <span class="math inline">\(f(X)\)</span>, to make predictions of y from those input variables. We also need a function to penalize errors in prediction, i.e. a <span class="emph">loss function</span>. With a chosen loss function, we then find the model which will minimize loss, generally speaking. We will start with the familiar and note a couple others that might be used.</p>
<div id="loss-functions" class="section level2">
<h2>Loss Functions</h2>
<div id="continuous-outcomes" class="section level3">
<h3>Continuous Outcomes</h3>
<div id="squared-error" class="section level4">
<h4>Squared Error</h4>
<p>The classic loss function for linear models with a continuous numeric response is the squared error loss function, or the residual sum of squares.</p>
<p><span class="math display">\[L(Y, f(X)) = \sum(y-f(X))^2\]</span></p>
<p>Everyone who has taken a statistics course is familiar with this ‘least-squares’ approach on some level. Often they are not taught that it is one of many possible approaches. However, the average, or <span class="emph">mean squared error</span> is commonly used as a metric of performance (or it’s square root).</p>
</div>
<div id="absolute-error" class="section level4">
<h4>Absolute Error</h4>
<p>For an approach that is more robust to extreme observations, we might choose absolute rather than squared error. In this case, predictions are a conditional median rather than a conditional mean.</p>
<p><span class="math display">\[L(Y, f(X)) = \sum|(y-f(X))|\]</span></p>
</div>
<div id="negative-log-likelihood" class="section level4">
<h4>Negative Log-likelihood</h4>
<p>We can also think of our usual likelihood methods learned in a standard applied statistics course<a href="#fn10" class="footnoteRef" id="fnref10"><sup>10</sup></a> as incorporating a loss function that is the <em>negative</em> log-likelihood pertaining to the model of interest. Such methods seek to maximize the likelihood of the data given the parameters. To turn it into a loss function, we simply minimize its negative value. As an example, if we assume a normal distribution for the response we can note the loss function as:</p>
<p><span class="math display">\[L(Y, f(X)) = n\ln{\sigma} + \sum \frac{1}{2\sigma^2}(y-f(X))^2\]</span></p>
<p>In this case it would converge to the same answer as the squared error/least squares solution.</p>
</div>
<div id="r-example" class="section level4">
<h4>R Example</h4>
<p>The following provides conceptual code that one could use with the <span class="func">optim</span> function in R to find estimates of regression coefficients (<span class="math inline">\(\beta\)</span> from before) based on minimizing the squared error. <code>X</code> is a design matrix of our predictor variables with the first column a vector of 1s in order to estimate the intercept. <code>y</code> is the continuous variable to be modeled. In the following, the true values for the intercept and other coefficients are <code>(0, -.5, .5)</code>. We can then compare the results with the <span class="func">lm</span> function from base R<a href="#fn11" class="footnoteRef" id="fnref11"><sup>11</sup></a>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sqerrloss =<span class="st"> </span><span class="cf">function</span>(beta, X, y){
  mu =<span class="st"> </span>X <span class="op">%*%</span><span class="st"> </span>beta
  <span class="kw">sum</span>((y<span class="op">-</span>mu)<span class="op">^</span><span class="dv">2</span>)
}

<span class="co"># data setup</span>
<span class="kw">set.seed</span>(<span class="dv">123</span>)                              <span class="co"># for reproducibility</span>
N =<span class="st"> </span><span class="dv">100</span>                                    <span class="co"># sample size</span>
X =<span class="st"> </span><span class="kw">cbind</span>(<span class="dv">1</span>, <span class="dt">X1=</span><span class="kw">rnorm</span>(N), <span class="dt">X2=</span><span class="kw">rnorm</span>(N))     <span class="co"># model matrix: intercept, 2 predictors</span>
beta =<span class="st"> </span><span class="kw">c</span>(<span class="dv">0</span>, <span class="op">-</span>.<span class="dv">5</span>, .<span class="dv">5</span>)                       <span class="co"># true coef values</span>
y =<span class="st">  </span><span class="kw">rnorm</span>(N, X<span class="op">%*%</span>beta, <span class="dt">sd=</span><span class="dv">1</span>)              <span class="co"># target</span>

<span class="co"># results</span>
our_func =<span class="st"> </span><span class="kw">optim</span>(<span class="dt">par=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>),             <span class="co"># starting values</span>
                 <span class="dt">fn=</span>sqerrloss, 
                 <span class="dt">X=</span>X, 
                 <span class="dt">y=</span>y, 
                 <span class="dt">method=</span><span class="st">&#39;BFGS&#39;</span>)
lm_result =<span class="st"> </span><span class="kw">lm</span>(y <span class="op">~</span><span class="st"> </span>., <span class="kw">data.frame</span>(X[,<span class="op">-</span><span class="dv">1</span>]))  <span class="co"># check with lm </span>

<span class="kw">rbind</span>(<span class="dt">optim=</span><span class="kw">c</span>(our_func<span class="op">$</span>par, our_func<span class="op">$</span>value), 
      <span class="dt">lm=</span><span class="kw">c</span>(<span class="kw">coef</span>(lm_result), <span class="dt">loss=</span><span class="kw">sum</span>(<span class="kw">resid</span>(lm_result)<span class="op">^</span><span class="dv">2</span>)))</code></pre></div>
<pre><code>      (Intercept)         X1        X2     loss
optim   0.1350654 -0.6331715 0.5238113 87.78187
lm      0.1350654 -0.6331715 0.5238113 87.78187</code></pre>
<p>While <span class="func">lm</span> uses a different approach, they are both going to result in the ‘least-squares’ estimates. Just to be clear, this is an exercise to understand how optimization works, not something you’d ever do for the standard regression setting. However, many ML tools will require you to supply your data as a model matrix, specify target(s), and select both a loss function and optimizer, and even possibly additional options for the latter.</p>
</div>
</div>
<div id="categorical-outcomes" class="section level3">
<h3>Categorical Outcomes</h3>
<p>Here we’ll also look at some loss functions useful in classification problems. Note that there is not necessary exclusion in loss functions for continuous vs. categorical outcomes<a href="#fn12" class="footnoteRef" id="fnref12"><sup>12</sup></a>. Generally though, we’ll have different options.</p>
<div id="misclassification" class="section level4">
<h4>Misclassification</h4>
<p>Probably the most straightforward is misclassification, or 0-1 loss. If we note <span class="math inline">\(f\)</span> as the prediction, and for convenience we assume a [-1,1] response instead of a [0,1] response:</p>
<p><span class="math display">\[L(Y, f(X)) = \sum I(y\neq \mathrm{sign}(f))\]</span></p>
<p>In the above, <span class="math inline">\(I\)</span> is the indicator function, and so we are simply summing misclassifications.</p>
</div>
<div id="binomial-log-likelihood" class="section level4">
<h4>Binomial log-likelihood</h4>
<p><span class="math display">\[L(Y, f(X)) = \sum \ln(1 + e^{-2yf})\]</span></p>
<p>The above is in deviance form. If you’re not familiar, deviance can conceptually be thought of as the GLM version of residual variance. This loss is equivalent to binomial log likelihood when <span class="math inline">\(y\)</span> is on the 0-1 scale.</p>
</div>
<div id="exponential" class="section level4">
<h4>Exponential</h4>
<p>Exponential loss is yet another loss function at our disposal.</p>
<p><span class="math display">\[L(Y, f(X)) = \sum e^{-yf}\]</span></p>
</div>
<div id="hinge-loss" class="section level4">
<h4>Hinge Loss</h4>
<p>A final loss function to consider, typically used with support vector machines, is the hinge loss function.</p>
<p><span class="math display">\[L(Y, f(X)) = \max(1-yf, 0)\]</span></p>
<p>Here negative values of <span class="math inline">\(yf\)</span> are misclassifications, and so correct classifications do not contribute to the loss. We could also note it as <span class="math inline">\(\sum (1-yf)_+\)</span> , i.e. summing only those positive values of <span class="math inline">\(1-yf\)</span>.</p>
<p>The following image compares these (from <span class="citation">Hastie, Tibshirani, and Friedman (<a href="#ref-hastie_elements_2009">2009</a>)</span>, fig. 10.4). As before we are assuming y is {-1,1}, and here f is our prediction, where positive values are classified as 1 and negative values are classified as -1. <span class="math inline">\(yf\)</span>, the margin, is akin to the residual from regression, where positive values indicate correct classification.</p>
<p><img src="img/lossfuncs.png" style="display:block; margin: 0 auto;" width=50%></p>
<p>Which of these might work best may be specific to the situation, but the gist is that they penalize negative values (misclassifications) more heavily and increasingly (except for misclassification error, which penalizes all misclassifications equally), with their primary difference in how heavy that penalty is.</p>
</div>
</div>
</div>
<div id="regularization" class="section level2">
<h2>Regularization</h2>
<p><span class="newthought">It is important to note</span> that a model fit to a single data set might do very well with the data at hand, but then suffer when predicting independent data<a href="#fn13" class="footnoteRef" id="fnref13"><sup>13</sup></a>. Also, oftentimes we are interested in a ‘best’ subset of predictors among a great many, and in this scenario the estimated coefficients are overly optimistic. This general issue can be improved by shrinking estimates toward zero, such that some of the performance in the initial fit is sacrificed for improvement with regard to prediction.</p>
<p>Penalized estimation will provide estimates with some shrinkage, and we can use it with little additional effort with our common procedures. Concretely, let’s apply this to the standard linear model, where we are finding estimates of <span class="math inline">\(\beta\)</span> that minimize the squared error loss.</p>
<p><span class="math display">\[\hat\beta = \underset{\beta}{\mathrm{arg\, min}} \sum{(y-X\beta)^2}\]</span></p>
<p>In words, we’re finding the coefficients that minimize the sum of the squared residuals. Now we just add a penalty component to the procedure as follows.</p>
<p><span class="math display">\[\hat\beta = \underset{\beta}{\mathrm{arg\, min}} \sum{(y-X\beta)^2} + \lambda\overset{p}{\underset{j=1}{\sum}}{\left|\beta_j\right|}\]</span></p>
<p>In the above equation, <span class="math inline">\(\lambda\)</span> is our penalty term<a href="#fn14" class="footnoteRef" id="fnref14"><sup>14</sup></a> for which larger values will result in more shrinkage. It’s applied to the <span class="math inline">\(L_1\)</span> or Manhattan norm of the coefficients, <span class="math inline">\(\beta_1,\beta_2...\beta_p\)</span>, i.e. <em>not including the intercept</em> <span class="math inline">\(\beta_0\)</span>, and is the sum of their absolute values (commonly referred to as the <span class="emph">lasso</span><a href="#fn15" class="footnoteRef" id="fnref15"><sup>15</sup></a>). For generalized linear and additive models, we can conceptually express a penalized likelihood as follows:</p>
<p><span class="math display">\[l_p(\beta) = l(\beta) - \lambda\overset{p}{\underset{j=1}{\sum}}{\left|\beta_j\right|}\]</span></p>
<p>As we are maximizing the likelihood, the penalty is a subtraction, but nothing inherently different is shown. If we are minimizing the negative (log) likelihood, we then add the penalty. This basic idea of adding a penalty term will be applied to all machine learning approaches, but as shown, we can apply such a tool to classical methods to boost prediction performance.</p>
<p>It should be noted that we can go about the regularization in different ways. For example, using the squared <span class="math inline">\(L_2\)</span> norm results in what is called <span class="emph"></span> (a.k.a. Tikhonov regularization)<a href="#fn16" class="footnoteRef" id="fnref16"><sup>16</sup></a>, and using a weighted combination of the lasso and ridge penalties gives us <span class="emph">elastic net</span> regularization. We’ll see an example of this later.</p>
<div id="r-example-1" class="section level3">
<h3>R Example</h3>
<p>In the following example, we take a look at the lasso approach for a standard linear model. We add the regularization component, with a fixed penalty <span class="math inline">\(\lambda\)</span> for demonstration purposes<a href="#fn17" class="footnoteRef" id="fnref17"><sup>17</sup></a>. However, you should insert your own values for <span class="math inline">\(\lambda\)</span> in the <span class="func">optim</span> line to see how the results are affected. I’ve also increased the number of predictors to 10.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># data setup</span>
<span class="kw">set.seed</span>(<span class="dv">123</span>)
N =<span class="st"> </span><span class="dv">100</span>
X =<span class="st"> </span><span class="kw">cbind</span>(<span class="dv">1</span>, <span class="kw">matrix</span>(<span class="kw">rnorm</span>(N<span class="op">*</span><span class="dv">10</span>), <span class="dt">ncol=</span><span class="dv">10</span>))
beta =<span class="st"> </span><span class="kw">runif</span>(<span class="kw">ncol</span>(X))
y =<span class="st">  </span><span class="kw">rnorm</span>(N, X<span class="op">%*%</span>beta, <span class="dt">sd=</span><span class="dv">2</span>)

sqerrloss_reg =<span class="st"> </span><span class="cf">function</span>(beta, X, y, <span class="dt">lambda=</span>.<span class="dv">5</span>){
  mu =<span class="st"> </span>X<span class="op">%*%</span>beta
  <span class="co"># sum((y-mu)^2) + lambda*sum(abs(beta[-1])) # conceptual</span>
  <span class="kw">sum</span>((y<span class="op">-</span>mu)<span class="op">^</span><span class="dv">2</span>) <span class="op">+</span><span class="st"> </span><span class="dv">2</span><span class="op">*</span><span class="kw">length</span>(y)<span class="op">*</span>lambda<span class="op">*</span><span class="kw">sum</span>(<span class="kw">abs</span>(beta[<span class="op">-</span><span class="dv">1</span>])) <span class="co"># actual for lasso</span>
}

lm_result =<span class="st"> </span><span class="kw">lm</span>(y<span class="op">~</span>., <span class="kw">data.frame</span>(X[,<span class="op">-</span><span class="dv">1</span>]) )
regularized_result =<span class="st"> </span><span class="kw">optim</span>(<span class="dt">par=</span><span class="kw">rep</span>(<span class="dv">0</span>, <span class="kw">ncol</span>(X)), 
                           <span class="dt">fn=</span>sqerrloss_reg, 
                           <span class="dt">X=</span>X, 
                           <span class="dt">y=</span>y, 
                           <span class="dt">method=</span><span class="st">&#39;BFGS&#39;</span>)</code></pre></div>
<p><br></p>
<p><div id="htmlwidget-ac291b881a17a3904e44" style="width:50%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-ac291b881a17a3904e44">{"x":{"filter":"none","data":[["(Intercept)","X1","X2","X3","X4","X5","X6","X7","X8","X9","X10"],[0.145,-0.27,0.586,0.385,0.844,0.511,0.423,0.304,-0.076,0.547,0.625],[0.114,-0.001,0.01,0.001,0.335,0.003,0.003,0,-0,0.001,0.165]],"container":"<table class=\"compact|nowrap\">\n  <thead>\n    <tr>\n      <th> <\/th>\n      <th>Standard LM<\/th>\n      <th>Regularized Model<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"dom":"t","pageLength":11,"ordering":false,"columnDefs":[{"className":"dt-right","targets":[1,2]},{"orderable":false,"targets":0}],"order":[],"autoWidth":false,"orderClasses":false,"lengthMenu":[10,11,25,50,100],"rowCallback":"function(row, data) {\nvar value=data[0]; if (value!==null) $(this.api().cell(row, 0).node()).css({'background-color':'#fffff8'});\nvar value=data[1]; if (value!==null) $(this.api().cell(row, 1).node()).css({'background-color':'#fffff8'});\nvar value=data[2]; if (value!==null) $(this.api().cell(row, 2).node()).css({'background-color':'#fffff8'});\nvar value=data[3]; if (value!==null) $(this.api().cell(row, 3).node()).css({'background-color':'#fffff8'});\n}"}},"evals":["options.rowCallback"],"jsHooks":[]}</script> <br></p>
<p>From the above, we can see in this case that the penalized coefficients have indeed shrunk toward zero. Now we move to testing. Normally both the training and test sets will be a random split of the original set, for this demo we’ll generate the test as we did with the training.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Create test data</span>
N_test =<span class="st"> </span><span class="dv">50</span>
X_test =<span class="st"> </span><span class="kw">cbind</span>(<span class="dv">1</span>, <span class="kw">matrix</span>(<span class="kw">rnorm</span>(N_test<span class="op">*</span><span class="dv">10</span>), <span class="dt">ncol=</span><span class="dv">10</span>))
y_test =<span class="st"> </span><span class="kw">rnorm</span>(N_test, X_test<span class="op">%*%</span>beta, <span class="dt">sd=</span><span class="dv">2</span>)

<span class="co"># fits on training set</span>
fits_lm =<span class="st"> </span><span class="kw">fitted</span>(lm_result, <span class="dt">newdata =</span> <span class="kw">data.frame</span>(X))
fits_reg =<span class="st"> </span>X<span class="op">%*%</span>regularized_result<span class="op">$</span>par

<span class="co"># loss on training set</span>
<span class="kw">data.frame</span>(<span class="dt">lm_train =</span> <span class="kw">crossprod</span>(y <span class="op">-</span><span class="st"> </span>fits_lm),
           <span class="dt">regularized_train =</span> <span class="kw">crossprod</span>(y <span class="op">-</span><span class="st"> </span>fits_reg))</code></pre></div>
<pre><code>  lm_train regularized_train
1 402.2046          583.3891</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># fits on test set</span>
fits_lm =<span class="st"> </span><span class="kw">predict</span>(lm_result, <span class="dt">newdata =</span> <span class="kw">data.frame</span>(X_test))
fits_reg =<span class="st"> </span>X_test<span class="op">%*%</span>regularized_result<span class="op">$</span>par

<span class="co"># loss on test set</span>
<span class="kw">data.frame</span>(<span class="dt">lm_test =</span> <span class="kw">crossprod</span>(y_test <span class="op">-</span><span class="st"> </span>fits_lm),
           <span class="dt">regularized_test =</span> <span class="kw">crossprod</span>(y_test <span class="op">-</span><span class="st"> </span>fits_reg))</code></pre></div>
<pre><code>   lm_test regularized_test
1 254.0681         232.4942</code></pre>
<p>We can see that the residual sum of squares has increased just a tad for the regularized fit on the training data. On the test data however, the squared error loss is lower.</p>
<p>In general, we can add the same sort of penalty to any number of models, such as logistic regression, neural net models, recommender systems etc. The primary goal again is to hopefully increase our ability to generalize the selected model to new data. Note that the estimates produced are in fact <em>biased</em>, but we have decreased the variance with new predictions as a counterbalance, and this brings us to the topic of the next section.</p>
</div>
</div>
<div id="bias-variance-tradeoff" class="section level2">
<h2>Bias-Variance Tradeoff</h2>
<p>In most of science, we are concerned with reducing uncertainty in our knowledge of some phenomenon. The more we know about the factors involved or related to some outcome of interest, the better we can predict that outcome upon the influx of new information. The initial step is to take the data at hand, and determine how well a model or set of models fit the data in various fashions. In many applications however, this part is also more or less the end of the game as well<a href="#fn18" class="footnoteRef" id="fnref18"><sup>18</sup></a>.</p>
<p>Unfortunately, such an approach, in which we only fit models to one data set, does not give a very good sense of <span class="emph">generalization</span> performance, i.e. the performance we would see with new data. While typically not reported, most researchers, if they are spending appropriate time with the data, are actually testing a great many models, among which the ‘best’ is then provided in detail in the end report. Without some generalization performance check however, such performance is overstated when it comes to new data.</p>
<p>In the following, consider a standard linear model scenario, e.g. with squared-error loss function and perhaps some regularization, and a data set in which we split the observations in some random fashion into a <span class="emph">training set</span>, for initial model fit, and a <span class="emph">test set</span>, which will be kept separate and independent, and used to measure generalization performance<a href="#fn19" class="footnoteRef" id="fnref19"><sup>19</sup></a>. We note <span class="emph">training error</span> as the (average) loss over the training set, and <span class="emph">test error</span> as the (average) prediction error obtained when a model resulting from the training data is fit to the test data. So, in addition to the previously noted goal of finding the ‘best’ model (<span class="emph">model selection</span>), we are interested further in estimating the prediction error with new data (<span class="emph">model performance</span>).</p>
<div id="bias-variance" class="section level3">
<h3>Bias &amp; Variance</h3>
<p>We start<a href="#fn20" class="footnoteRef" id="fnref20"><sup>20</sup></a> with a true data generating process for some target <span class="math inline">\(y\)</span>, expressed as a function of features <span class="math inline">\(X\)</span>. We can specify the true model as</p>
<p><span class="math display">\[y = f(X) + \epsilon\]</span></p>
<p>where <span class="math inline">\(f(x)\)</span> is the expected value of <span class="math inline">\(y\)</span> given <span class="math inline">\(X\)</span>, i.e. <span class="math inline">\(f(x) = E(y|X)\)</span>. The expected value of the error, <span class="math inline">\(E(\epsilon)=0\)</span>, has some variance, <span class="math inline">\(\textrm{Var}(\epsilon) = \sigma^2_\epsilon\)</span>. In other words, we are talking about the standard regression model we all know and love. Now we can conceptually think of the <em>expected prediction error</em> at a specific input <span class="math inline">\(X = x_*\)</span> as:</p>
<p><span class="math display">\[\text{Error}_{x_*} = \text{Irreducible Error} + \text{Bias}^2 + \text{Variance}\]</span></p>
<p>To better understand this, think of training models over and over, each time with new training data, but testing each model at input <span class="math inline">\(x_*\)</span>. The <span class="math inline">\(\text{Error}_{x_*}\)</span> is the average, or expected value of the prediction error in this scenario, or <span class="math inline">\(E[(y - \hat f(x))^2|X=x_*]\)</span>, with <span class="math inline">\(\hat f\)</span> our current estimate of the true underlying data generating function <span class="math inline">\(f\)</span>. We can note three components to this general notion of prediction error:</p>
<p><strong>Irreducible error</strong>: The variance of the (new test) target (<span class="math inline">\(\sigma^2_\epsilon\)</span>). This is unavoidable, since our <span class="math inline">\(y\)</span> is measured with error.</p>
<p><strong>Bias<sup>2</sup></strong>: the amount the <em>average</em> of our estimate varies from the true (but unknown) value (<span class="math inline">\(E(\hat f) - f\)</span>). This is often the result of trying to model the complexity of nature with something much simpler that the human brain can understand. While the simpler might make us feel good, it may not work very well.</p>
<p><strong>Variance</strong>: the amount by which our prediction would change if we had estimated it using a different training data set (<span class="math inline">\(Var(\hat f)\)</span>). Even with unbiased estimates, we could still see a high mean squared error due to high variance.</p>
<p>Slightly more formally, we can present this as follows, with <span class="math inline">\(h_*\)</span> our estimated (hypothesized) value at <span class="math inline">\(x_*\)</span>:</p>
<p><span class="math display">\[\text{Error}_{x_*} = \text{Var}(\epsilon) + (\text{E}[h_*] - f(x_*))^2 + \text{Var}(h_*)\]</span></p>
<p>The latter two components make up the mean squared error in our previous demonstration. While they are under our control, they compete with one another such that oftentimes we improve one at the detriment of the other. In other words, <em>bias and variance are not independent</em>.</p>
</div>
<div id="the-tradeoff" class="section level3">
<h3>The Tradeoff</h3>
<p>Outlining a general procedure, we start by noting the prediction error on a training data set with multiple models of varying complexity (e.g. increasing the number of predictor variables, adding polynomial terms, including interactions), and then assess the performance of the chosen models in terms of prediction error on the test set. We then perform the same activity for a total of 100 simulated data sets, for each level of complexity.</p>
<p>The results from this process might look like the following, taken from <span class="citation">Hastie, Tibshirani, and Friedman (<a href="#ref-hastie_elements_2009">2009</a>)</span>. With regard to the training data, we have <span class="math inline">\(\mathrm{error}_{\mathrm{train}}\)</span> for one hundred training sets for each level of model complexity. The bold blue line notes this average error over the 100 sets by model complexity, and we can see that more complex models fit the data better. The bold red line is the average test error (<span class="math inline">\(\mathrm{error}_{\mathrm{test}}\)</span>) across the 100 test data sets, and it tells a different story. When models get too complex, the test error starts to <em>increase</em>.</p>
<p><img src="img/biasvar2.svg" style="display:block; margin: 0 auto;" width=50%></p>
<p>Ideally we’d like to see low bias and (relatively) low variance, but things are not so easy. One thing we can see clearly is that <span class="math inline">\(\mathrm{error}_{\mathrm{train}}\)</span> is not a good estimate of <span class="math inline">\(\mathrm{error}_{\mathrm{test}}\)</span>, which is now our focus in terms of performance. If we think of the training error as what we would see in typical research where one does everything with a single data set, we are using the same data set to fit the model and assess error. As the model is adapted to that data set specifically, it will be overly optimistic in the estimate of the error, that optimism being the difference between the error rate we see based on the training data versus the average of what we would get with many test data sets. We can think of this as a problem of overfitting to the training data. Models that do not incorporate any regularization or validation process of any kind are likely overfit to the data presented.</p>
<p>Generally speaking, the more complex the model, the lower the bias, but the higher the variance, as depicted in the graphic. Specifically however, the situation is more nuanced, where the type of problem (classification with 0-1 loss vs. continuous response with squared error loss<a href="#fn21" class="footnoteRef" id="fnref21"><sup>21</sup></a>) and technique (a standard linear model vs. regularized fit) will exhibit different bias-variance relationships.</p>
</div>
<div id="diagnosing-bias-variance-issues-possible-solutions" class="section level3">
<h3>Diagnosing Bias-Variance Issues <em>&amp;</em> Possible Solutions</h3>
<p>The following can serve as a visual summary of the concepts just outlined (figure adapted from <span class="citation">Domingos (<a href="#ref-domingos_few_2012">2012</a>)</span>).</p>
<p><img src="img/biasvartarget.svg" style="display:block; margin: 0 auto;" width=50%> <br></p>
<p>Now let’s assume a regularized linear model with a standard data split into training and test sets. We will describe different scenarios with possible solutions.</p>
<div id="worst-case-scenario" class="section level4">
<h4>Worst Case Scenario</h4>
<p>Starting with the worst case scenario, poor models may exhibit high bias and high variance. One thing that will not help this situation (perhaps contrary to intuition) is adding more data. You can’t make a silk purse out of a sow’s ear (<a href="https://libraries.mit.edu/archives/exhibits/purse/"><em>usually</em></a>), and adding more data just gives you a more accurate picture of how awful your model is. One might need to rework the model, e.g. adding new predictors or creating them via interaction terms, polynomials, or other smooth functions as in additive models, or simply collecting better and/or more relevant data.</p>
</div>
<div id="high-variance" class="section level4">
<h4>High Variance</h4>
<p>When variance is a problem, our training error is low while test error is relatively high (overfitting problem). Implementing more shrinkage or other penalization to model complexity may help with the issue. In this case more data may help as well.</p>
</div>
<div id="high-bias" class="section level4">
<h4>High Bias</h4>
<p>With bias issues, our training error is high and test error is not too different from training error (underfitting problem). Adding new predictors/features, e.g. interaction terms, polynomials etc., can help here. Additionally, reducing the penalty parameter <span class="math inline">\(\lambda\)</span> would also work with even less effort, though generally it should be estimated rather than explicitly set.</p>
<p>Here is another visualization to drive the point home.</p>
<p><img class='imgbigger' src="img/biasvar_gp.svg" style="display:block; margin: 0 auto;" width=50%></p>
<p><br></p>
<p>The figure is inspired by <span class="citation">Murphy (<a href="#ref-murphy_machine_2012">2012</a>)</span> (figure 6.5) showing the bias-variance trade-off. Sample (left) and average (right) fits of linear regression using a Gaussian radial basis function expansion. The blue line represents the true x-y relationship. The top row shows low variance between one fit and the next (left) but notable bias (right) in that the average fit is off. Compare to the less regularized (high variance, low bias) situation of the bottom row. See the <span class="pack">kernlab</span> package for the fitting function used, and the <a href="appendix.html#appendix">appendix</a> for the code used to produce the graph.</p>
</div>
</div>
<div id="bias-variance-summary" class="section level3">
<h3>Bias-Variance Summary</h3>
<p>One of the key ideas any applied researcher can take from machine learning concerns the bias-variance trade-off and issues of overfitting in particular. Typical applied practice involves potentially dozens of models fit to the same data set without any validation whatsoever, yet only one or two are actually presented in publication. Many disciplines report nothing but the statistical significance, and yet one can have statistically significant predictors and have predictive capability that is no different from guessing. Furthermore, very complex models are often fit to small data sets, compounding the problem.</p>
<p>It is very easy to describe <strong><em>science</em></strong> without ever talking about statistical significance. It is impossible to talk about science without talking about prediction. The bias-variance trade-off is one way to bring the concerns of prediction to the forefront, and any applied researcher can benefit from thinking about its implications<a href="#fn22" class="footnoteRef" id="fnref22"><sup>22</sup></a>.</p>
</div>
</div>
<div id="cross-validation" class="section level2">
<h2>Cross-Validation</h2>
<p>As noted in the previous section, in machine learning approaches we are particularly concerned with prediction error on new data. The simplest validation approach would be to split the data available into a training and test set as discussed previously. We estimate the model on the training data, and apply the model to the test data, get the predictions and measure our test error, selecting whichever model results in the least test error. The following displays a hypothetical learning curve from the results of such a process. While the approach is fairly simple, other approaches are more commonly used and result in better estimates of predictive performance<a href="#fn23" class="footnoteRef" id="fnref23"><sup>23</sup></a>.</p>
<p><img src="img/learningcurve.svg" style="display:block; margin: 0 auto;" width=50%></p>
<div id="adding-another-validation-set" class="section level3">
<h3>Adding Another Validation Set</h3>
<p>One technique that might be utilized for larger data sets, is to split the data into training, <em>validation</em>, and test sets. For example, one might take the original data and create something like a 60-20-20% split to create the needed data sets. The purpose of the initial validation set is to select the optimal model and determine the values of <span class="emph">tuning parameters</span>. These are parameters which generally deal with how complex a model one will allow, but for which one would have little inkling as to what they should be set at beforehand. For example, our <span class="math inline">\(\lambda\)</span> shrinkage parameter in regularized regression would be such a parameter. We first select models/tuning parameters that minimize the validation set error, and once the model is chosen, we then examine test set error performance. In this way performance assessment is still independent of the model development process.</p>
</div>
<div id="k-fold-cross-validation" class="section level3">
<h3>K-fold Cross-Validation</h3>
<p>In many cases we don’t have enough data for such a split, and the split percentages are arbitrary anyway, with results that would be specific to the split chosen. Instead we can take a typical data set and randomly split it into <span class="math inline">\(\kappa=10\)</span> equal-sized (or close to it) parts. Next, we take the first nine partitions, combine them, and use them as the training set. With chosen model from the training data, make predictions on the held-out partition. Now we do it again, but this time use the 9<sup>th</sup> partition as the holdout set. Repeat the process until each of the initial 10 partitions of data have been used as the test set. Average the error across all procedures for our estimate of prediction error. With enough data, this (and the following methods) could be used as the validation procedure before eventual performance assessment on an independent test set with the final chosen model.</p>
<img src="img/kfold.svg" style="display:block; margin: 0 auto;" width=50%> <br><br />

<p style="text-align:center">
An illustration of 3-fold classification.
</p>
<div id="leave-one-out-cross-validation" class="section level4">
<h4>Leave-one-out Cross-Validation</h4>
<p>Leave-one-out (LOO) cross-validation is the same thing but where <span class="math inline">\(\kappa=N\)</span>. In other words, we train a model for all observations except the <span class="math inline">\(\kappa^{th}\)</span> one, assessing fit on the observation that was left out. We then cycle through until all observations have been left out once to obtain an average accuracy.</p>
<p>Of the two, K-fold may have relatively higher bias but less variance, while LOO would have the converse problem, as well as possible computational issues<a href="#fn24" class="footnoteRef" id="fnref24"><sup>24</sup></a>. K-fold’s additional bias would be diminished would with increasing sample sizes, and generally 5 or 10-fold cross-validation is recommended. However, many model selection techniques (e.g. via AIC) have a leave-one-out interpretation.</p>
</div>
</div>
<div id="bootstrap" class="section level3">
<h3>Bootstrap</h3>
<p>With a bootstrap approach, we draw <span class="math inline">\(B\)</span> random samples with replacement from our original data set, creating <span class="math inline">\(B\)</span> bootstrapped data sets of the same size as the original data. We use the <span class="math inline">\(B\)</span> data sets as training sets and, using the original data as the test set, average the prediction error across the models.</p>
</div>
<div id="other-stuff" class="section level3">
<h3>Other Stuff</h3>
<p>Along with the above there are variations such as repeated cross validation, the ‘.632’ bootstrap and so forth. One would want to do a bit of investigating, but <span class="math inline">\(\kappa\)</span>-fold and bootstrap approaches generally perform well. If variable selection is part of the goal, one should be selecting subsets of predictors as part of the cross-validation process, not at some initial data step.</p>

</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-hastie_elements_2009">
<p>Hastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2009. <em>The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition</em>. 2nd ed. 2017. Corr. 12th Printing. Springer.</p>
</div>
<div id="ref-domingos_few_2012">
<p>Domingos, Pedro. 2012. “A Few Useful Things to Know About Machine Learning.” <em>Commun. ACM</em> 55 (10). doi:<a href="https://doi.org/10.1145/2347736.2347755">10.1145/2347736.2347755</a>.</p>
</div>
<div id="ref-murphy_machine_2012">
<p>Murphy, Kevin P. 2012. <em>Machine Learning: A Probabilistic Perspective</em>. The MIT Press.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="10">
<li id="fn10"><p>Well some of you. Many applied methods courses don’t teach the basic maximum likelihood approach, even though it’s the most widely used estimation technique for statistical inference.<a href="concepts.html#fnref10">↩</a></p></li>
<li id="fn11"><p>Type <code>?optim</code> at the console for more detail.<a href="concepts.html#fnref11">↩</a></p></li>
<li id="fn12"><p>For example, if dealing with probabilities, we technically could use minimize squared errors in the case of classification also. We could use a maximum likelihood for either setting (or minimize the negative log likelihood to turn it into a loss function).<a href="concepts.html#fnref12">↩</a></p></li>
<li id="fn13"><p>In terminology we will discuss further later, such models might have low bias but notable variance.<a href="concepts.html#fnref13">↩</a></p></li>
<li id="fn14"><p>This can be set explicitly or also estimated via a validation approach. As we do not know it beforehand, we can estimate it on a validation data set (not the test set) and then use the estimated value when estimating coefficients via cross-validation with the test set. We will talk more about validation later.<a href="concepts.html#fnref14">↩</a></p></li>
<li id="fn15"><p>See Tibshirani (1996) Regression shrinkage and selection via the lasso.<a href="concepts.html#fnref15">↩</a></p></li>
<li id="fn16"><p>Interestingly, the lasso and ridge regression results can be seen as a Bayesian approach using a zero mean Laplace and Normal prior distribution respectively for the <span class="math inline">\(\beta_j\)</span>.<a href="concepts.html#fnref16">↩</a></p></li>
<li id="fn17"><p>As noted previously, in practice <span class="math inline">\(\lambda\)</span> would be estimated via some validation procedure.<a href="concepts.html#fnref17">↩</a></p></li>
<li id="fn18"><p>I should note that I do not make any particular claim about the quality of such analysis. In many situations, the cost of data collection is very high, and for all the current enamorment with ‘big’ data, a lot of folks will never have access to big data for their situation (e.g. certain clinical populations). In these situations, getting new data for which one might make predictions is extremely difficult.<a href="concepts.html#fnref18">↩</a></p></li>
<li id="fn19"><p>In typical settings, there are parameters specific to some analytical technique for which one would have no knowledge, and which must be estimated along with the usual parameters of the standard models. The <span class="math inline">\(\lambda\)</span> penalty parameter in regularized regression is one example of such a <span class="emph">tuning parameter</span>. In the best case scenario, we would also have a <span class="emph">validation set</span>, where we could determine appropriate values for such parameters based on performance with the validation data set, and then assess generalization performance on the test set when the final model has been chosen. However, methods are available to us in which we can approximate the validation step in other ways.<a href="concepts.html#fnref19">↩</a></p></li>
<li id="fn20"><p>Much of the following is essentially a paraphrase of parts of <span class="citation">Hastie, Tibshirani, and Friedman (<a href="#ref-hastie_elements_2009">2009</a>)</span> (chapters 2 and 7).<a href="concepts.html#fnref20">↩</a></p></li>
<li id="fn21"><p>See Friedman (1996) <em>On Bias, Variance, 0/1 Loss and the Curse of Dimensionality</em> for the unusual situations that can arise in dealing with classification error with regard to bias and variance.<a href="concepts.html#fnref21">↩</a></p></li>
<li id="fn22"><p>Note that the bias-variance tradeoff doesn’t readily apply to classification problems, at least not in the same way. To begin, the relationship is multiplicative, and depending on which side of the decision boundary one is on (correct vs. not), it might even be useful to increase the variance. However, if one thinks in terms of predicted probabilities rather than predicted class, one can more or less keep the same mindset.<a href="concepts.html#fnref22">↩</a></p></li>
<li id="fn23"><p>Along with some of the other works cited, see <span class="citation">Harrell (<a href="#ref-harrell2015regression">2015</a>)</span> for a good discussion of model validation.<a href="concepts.html#fnref23">↩</a></p></li>
<li id="fn24"><p>For squared-error loss situations, there is a Generalized cross-validation (GCV) that can be estimated more directly without actually going to the entire LOO procedure, and functions similarly to AIC.<a href="concepts.html#fnref24">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="introduction.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="opening-the-black-box.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": false,
"twitter": false,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["twitter", "facebook", "google", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": null,
"download": null,
"toc": {
"collapse": "section",
"depth": 2,
"scroll_highlight": true
},
"highlight": "pygments",
"search": true
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
